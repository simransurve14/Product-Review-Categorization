{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45a6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('sarcasm_detection_dataset.csv',encoding='latin-1')\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = torch.tensor([1.0 / count for count in Counter(df[\"Category\"]).values()])\n",
    "\n",
    "# Map category labels to integers\n",
    "label_map = {label: i for i, label in enumerate(df[\"Category\"].unique())}\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd6cc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sarcasm': 0, 'not sarcasm': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca9f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.dataframe.iloc[idx][\"Review\"]\n",
    "        category = self.dataframe.iloc[idx][\"Category\"]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        label = torch.tensor(label_map[category])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": label,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54095814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 6563\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2463' max='2463' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2463/2463 10:37:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./sarcasm_results\\checkpoint-500\n",
      "Configuration saved in ./sarcasm_results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./sarcasm_results\\checkpoint-500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./sarcasm_results\\checkpoint-1000\n",
      "Configuration saved in ./sarcasm_results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./sarcasm_results\\checkpoint-1000\\pytorch_model.bin\n",
      "Saving model checkpoint to ./sarcasm_results\\checkpoint-1500\n",
      "Configuration saved in ./sarcasm_results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./sarcasm_results\\checkpoint-1500\\pytorch_model.bin\n",
      "Saving model checkpoint to ./sarcasm_results\\checkpoint-2000\n",
      "Configuration saved in ./sarcasm_results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./sarcasm_results\\checkpoint-2000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1641\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='206' max='206' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [206/206 20:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.043757785111665726, 'eval_accuracy': 0.9914686166971359, 'eval_f1': 0.9914686483786789, 'eval_precision': 0.9914716255206522, 'eval_recall': 0.9914686166971359, 'eval_runtime': 1244.9341, 'eval_samples_per_second': 1.318, 'eval_steps_per_second': 0.165, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "hf_token='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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nikesh66/Sarcasm-Detection-using-BERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nikesh66/Sarcasm-Detection-using-BERT\", num_labels=len(label_map))\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sarcasm_results\",  # Specify the output directory\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = ReviewDataset(train_df, tokenizer, max_length=128)\n",
    "test_dataset = ReviewDataset(test_df, tokenizer, max_length=128)\n",
    "\n",
    "# Compute weights for each sample in the dataset\n",
    "class_weights = [class_weights[label_map[label]] for label in train_df[\"Category\"]]\n",
    "sampler = WeightedRandomSampler(class_weights, len(train_dataset), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f734b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./sarcasm_results\n",
      "Configuration saved in ./sarcasm_results\\config.json\n",
      "Model weights saved in ./sarcasm_results\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ca13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.21.0\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10001854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModelForSequenceClassification\n",
    "label_map={'sarcasm': 0, 'not sarcasm': 1}\n",
    "model1=TFAutoModelForSequenceClassification.from_pretrained(\"sarcasm_results\", num_labels=len(label_map),from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nikesh66/Sarcasm-Detection-using-BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379ef6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter review: PM modi is taking much more initiatives since elections are on the way.\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "predicted index 1\n",
      "Predicted category: not sarcasm\n"
     ]
    }
   ],
   "source": [
    "user_review = input('Enter review: ')\n",
    "\n",
    "# Tokenize the user review (assuming you have a tokenizer)\n",
    "tokenized_user_review = tokenizer(user_review, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model1.predict(dict(tokenized_user_review))\n",
    "\n",
    "# Extract the predicted probabilities for each class\n",
    "predicted_probabilities = predictions[0][0]\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class_index = predicted_probabilities.argmax()\n",
    "\n",
    "def print_key_by_value(dict, value1):\n",
    "  for key, value in dict.items():\n",
    "    if value == value1:\n",
    "      return key\n",
    "\n",
    "# Example usage:\n",
    "predicted_category=print_key_by_value(label_map,predicted_class_index )\n",
    "# Print the predicted class index\n",
    "print('predicted index',predicted_class_index)\n",
    "print(\"Predicted category:\", predicted_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
